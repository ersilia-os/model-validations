{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ed7ef1-0e07-4a9c-a8ab-e714ffbfb7cb",
   "metadata": {},
   "source": [
    "## This notebook is to check the model reproducibility with a sample from it's original publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55b984cd-75bb-49ed-b36e-f053e69d33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "DATAPATH = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c736f4-3c0c-44b2-a162-716dfa251674",
   "metadata": {},
   "source": [
    "## eos2ta5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b950c2f-4d23-43a8-bba7-9d0c48ab8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.5993902797701955\n",
      "NPV: 0.6875\n",
      "ACC: 0.8181818181818182\n",
      "PPV: 0.8928571428571429\n",
      "SPE: 0.7857142857142857\n",
      "SEN: 0.8333333333333334\n",
      "B-ACC: 0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset and the predictions dataset\n",
    "test_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"model_datasets\", \"eos2ta5_Test-set-I.csv\"))\n",
    "predictions_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"predictions_data\", \"reproducibility_predictions_eos2ta5.csv\"))\n",
    "\n",
    "# the predictions are in a column named 'probability' in the predictions dataset\n",
    "predicted_probabilities = predictions_df['probability']\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "predicted_labels = (predicted_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Extract the ground truth labels from the test dataset\n",
    "test_labels = test_df['ACTIVITY']  # 'ACTIVITY' column contains the ground truth labels\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Matthews correlation coefficient\n",
    "mcc = matthews_corrcoef(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "# Calculate Specificity (SPE)\n",
    "spe = tn / (tn + fp)\n",
    "\n",
    "# Print the results\n",
    "print(\"MCC:\", mcc)\n",
    "print(\"NPV:\", npv)\n",
    "print(\"ACC:\", accuracy)\n",
    "print(\"PPV:\", precision)\n",
    "print(\"SPE:\", spe)\n",
    "print(\"SEN:\", recall)\n",
    "print(\"B-ACC:\", balanced_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25743a00-2944-4bb2-a67a-0b12b62124bf",
   "metadata": {},
   "source": [
    "### Results according to the publication\n",
    "<img src=\"../figures/eos2ta5.png\" alt=\"eos2ta5_publication_result\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871244c-16f2-41ef-9884-c461f49421fc",
   "metadata": {},
   "source": [
    "### eos2ta5 has the same results as the publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ad7d7-d5a5-4a78-88c1-da3f5b99ccd4",
   "metadata": {},
   "source": [
    "## eos4tcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa3ad997-4b1f-4b35-b609-2da5f6fbf5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEN: 0.8333333333333334\n",
      "SPE: 0.7857142857142857\n",
      "MCC: 0.5993902797701955\n",
      "B-ACC: 0.8095238095238095\n",
      "F1: 0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Load the test dataset and the predictions dataset\n",
    "test_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"model_datasets\", \"eos4tcc_EX1.csv\"))\n",
    "predictions_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"predictions_data\", \"reproducibility_predictions_eos4tcc.csv\"))\n",
    "\n",
    "# the predictions are in a column named 'score' in the predictions dataset\n",
    "predicted_probabilities = predictions_df['score']\n",
    "\n",
    "# Define a threshold for converting probabilities to binary predictions\n",
    "threshold = 0.5  # You may adjust this threshold based on your preference or specific requirements\n",
    "\n",
    "# Convert probabilities to binary predictions based on the threshold\n",
    "binary_predictions = (predicted_probabilities >= threshold).astype(int)\n",
    "\n",
    "# Extract the ground truth labels from the test dataset\n",
    "test_labels = test_df['label']  # 'label' column contains the ground truth labels\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Matthews correlation coefficient\n",
    "mcc = matthews_corrcoef(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "# Calculate Specificity (SPE)\n",
    "spe = tn / (tn + fp)\n",
    "\n",
    "# Print the results\n",
    "print(\"SEN:\", recall)\n",
    "print(\"SPE:\", spe)\n",
    "print(\"MCC:\", mcc)\n",
    "print(\"B-ACC:\", balanced_accuracy)\n",
    "print(\"F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de3861-d81e-4d06-a84a-bac37cdbc9ec",
   "metadata": {},
   "source": [
    "### Results according to the publication\n",
    "<img src=\"../figures/eos4tcc.png\" alt=\"eos4tcc_publication_result\">\n",
    "\n",
    "<img src=\"../figures/eos4tcc2.png\" alt=\"eos4tcc_publication_result\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30889a-cb92-4ff4-8e6c-9b3ad8801c12",
   "metadata": {},
   "source": [
    "### it can be seen that the publication and eos4tcc have the same range of values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d11a10-5826-463b-8551-1f60fa702b70",
   "metadata": {},
   "source": [
    "## eos30gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe0b267-de00-4987-a1e6-19a5bc1b7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheet names: dict_keys(['Training set', 'Test set', 'Validation set'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the entire Excel file\n",
    "full_excel_data = pd.read_excel(os.path.join(DATAPATH, \"model_reproducibility\", \"model_datasets\", \"eos30gr_TableS4.xlsx\"), sheet_name=None)\n",
    "\n",
    "# Print the available sheet names\n",
    "sheet_names = full_excel_data.keys()  # Get the keys of the dictionary, which are the sheet names\n",
    "print(\"Available sheet names:\", sheet_names)\n",
    "\n",
    "# Access the 'Validation set' sheet from the dictionary\n",
    "validation_set_df = full_excel_data['Validation set']\n",
    "\n",
    "# Specify the path where you want to save the downloaded CSV file\n",
    "output_csv_path = os.path.join(DATAPATH, \"model_reproducibility\", \"model_datasets\", \"eos30gr_validation_set.csv\")  # Specify the full path to save the CSV file\n",
    "\n",
    "# Save the 'Validation set' sheet as a separate CSV file\n",
    "validation_set_df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51650b9b-79ba-4c2e-9a63-730519c2ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (SE): 0.9977011494252873\n",
      "Specificity (SP): 0.9965397923875432\n",
      "Positive Predictive Value (Q+): 1.0\n",
      "Negative Predictive Value (Q-): 0.9988439306358381\n",
      "Overall Accuracy (Q): 0.9987325728770595\n",
      "Area Under the Curve (AUC): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Load the test dataset and the predictions dataset\n",
    "test_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"model_datasets\", \"eos30gr_validation_set.csv\"))\n",
    "\n",
    "# Load the predictions dataset\n",
    "predictions_df = pd.read_csv(os.path.join(DATAPATH, \"model_reproducibility\", \"predictions_data\", \"reproducibility_predictions_eos30gr.csv\"))\n",
    "\n",
    "# Drop rows with NaN values from the 'activity (blockers‘ IC50 ≤ 10 μM; decoys‘ IC50 ＞10 μM)' column in the test dataset\n",
    "test_df = test_df.dropna(subset=['activity (blockers‘ IC50 ≤ 10 μM; decoys‘ IC50 ＞10 μM)'])\n",
    "\n",
    "# Drop corresponding rows from predictions_df\n",
    "predictions_df = predictions_df.loc[test_df.index]\n",
    "\n",
    "\n",
    "# The predictions are in the column named 'activity10'\n",
    "predicted_probabilities = predictions_df['activity10']\n",
    "\n",
    "# Define a threshold for converting probabilities to binary predictions\n",
    "threshold = 0.5  # You may adjust this threshold based on your preference or specific requirements\n",
    "\n",
    "# Convert probabilities to binary predictions based on the threshold\n",
    "binary_predictions = (predicted_probabilities >= threshold).astype(int)\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "predicted_labels = (predicted_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Extract the ground truth labels from the test dataset\n",
    "test_labels = test_df['activity (blockers‘ IC50 ≤ 10 μM; decoys‘ IC50 ＞10 μM)']  # 'label' column contains the ground truth labels\n",
    "\n",
    "# Calculate Sensitivity (SE)\n",
    "SE = recall_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Specificity (SP)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "SP = TN / (TN + FP)\n",
    "\n",
    "# Calculate Positive Predictive Value (Q+)\n",
    "Q_plus = precision_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Negative Predictive Value (Q-)\n",
    "Q_minus = TN / (TN + FN)\n",
    "\n",
    "# Calculate Overall Accuracy (Q)\n",
    "Q = accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "# Calculate Area Under the Curve (AUC)\n",
    "AUC = roc_auc_score(test_labels, predicted_probabilities)\n",
    "\n",
    "# Print the results\n",
    "print(\"Sensitivity (SE):\", SE)\n",
    "print(\"Specificity (SP):\", SP)\n",
    "print(\"Positive Predictive Value (Q+):\", Q_plus)\n",
    "print(\"Negative Predictive Value (Q-):\", Q_minus)\n",
    "print(\"Overall Accuracy (Q):\", Q)\n",
    "print(\"Area Under the Curve (AUC):\", AUC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d9eb6-ca39-492c-9b99-b5e3bab67655",
   "metadata": {},
   "source": [
    "### results of the publication\n",
    "<img src=\"../figures/eos30gr.png\" alt=\"eos30gr_publication_result\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bc8b0-bf96-4438-aff9-cbf4c3b062e9",
   "metadata": {},
   "source": [
    "## eos30f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147b3af-e528-42d9-9f9a-95e15f8c9f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
